{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica Modulo Machine Learning -- Daniel Alvaro V3\n",
    "\n",
    "En este notebook vamos a abordar la práctica del modulo de Machine Learning siguiendo los siguientes pasos.\n",
    "\n",
    "1. Generación de nuevas características a partir de las existentes\n",
    "2. Codificación de variables\n",
    "3. Análisis exploratorio\n",
    "4. Selección y evaluación del modelo\n",
    "5. Comparativa de distintos algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version V3\n",
    "En la version V3 se llevan a cabo los siguientes cambios:\n",
    "- Se lleva a cabo el analisis de outliers de la columna de precios (>10, <200). Con lo que conseguimos unos valores razonables de los modelos Ridge y Lasso (lineal y cuadratico)\n",
    "- Se utiliza el Main Encoder calculado en train para los datos de test (las categorias nuevas que puedan aparecer se rellenan con la media de las medias)\n",
    "- Eliminamos la prueba de la transformacion logaritmica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías y funciones\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "   \n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "    \n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    \n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    \n",
    "    # Sort the table by percentage of missing descending\n",
    "    # .iloc[:, 1]!= 0: filter on missing missing values not equal to zero\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "    '% of Total Values', ascending=False).round(2)  # round(2), keep 2 digits\n",
    "    \n",
    "    # Print some summary information\n",
    "    print(\"Your slelected dataframe has {} columns.\".format(df.shape[1]) + '\\n' + \n",
    "    \"There are {} columns that have missing values.\".format(mis_val_table_ren_columns.shape[0]))\n",
    "    \n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change not main categories to \"Other\", this funtion will be used in train and test to keep coherence between datasets\n",
    "def change_cat_to_other(array_main_cat, df):\n",
    "    array_categories = list(df.value_counts().index)\n",
    "    array_others = [x for x in array_categories if x not in array_main_cat]\n",
    "    return df.replace(array_others, 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de entrada\n",
    "\n",
    "Cargamos los datos del fichero de airbnb reducido.\n",
    "Este fichero contiene 14870 observaciones y 89 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_airbnb = pd.read_csv('./data/airbnb-listings.csv',sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14780, 89)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_airbnb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de nada, vamos a hacer una pequeña limpieza de los outliers de precio, para ver si asi obtenemos unos modelos mas razonables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_airbnb = full_airbnb[(full_airbnb['Price']>10) & (full_airbnb['Price']<200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your slelected dataframe has 89 columns.\n",
      "There are 58 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Has Availability</td>\n",
       "      <td>14049</td>\n",
       "      <td>99.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Acceptance Rate</td>\n",
       "      <td>14027</td>\n",
       "      <td>99.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jurisdiction Names</td>\n",
       "      <td>13858</td>\n",
       "      <td>98.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>License</td>\n",
       "      <td>13734</td>\n",
       "      <td>97.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Square Feet</td>\n",
       "      <td>13487</td>\n",
       "      <td>95.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Monthly Price</td>\n",
       "      <td>10603</td>\n",
       "      <td>75.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Weekly Price</td>\n",
       "      <td>10490</td>\n",
       "      <td>74.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Notes</td>\n",
       "      <td>8676</td>\n",
       "      <td>61.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Security Deposit</td>\n",
       "      <td>8091</td>\n",
       "      <td>57.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Interaction</td>\n",
       "      <td>6100</td>\n",
       "      <td>43.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Access</td>\n",
       "      <td>6022</td>\n",
       "      <td>42.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cleaning Fee</td>\n",
       "      <td>5755</td>\n",
       "      <td>40.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Transit</td>\n",
       "      <td>5298</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neighborhood Overview</td>\n",
       "      <td>5266</td>\n",
       "      <td>37.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host About</td>\n",
       "      <td>4981</td>\n",
       "      <td>35.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neighbourhood</td>\n",
       "      <td>4900</td>\n",
       "      <td>34.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>House Rules</td>\n",
       "      <td>4886</td>\n",
       "      <td>34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Space</td>\n",
       "      <td>3683</td>\n",
       "      <td>26.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Neighbourhood</td>\n",
       "      <td>3614</td>\n",
       "      <td>25.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Value</td>\n",
       "      <td>3028</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Location</td>\n",
       "      <td>3028</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Checkin</td>\n",
       "      <td>3024</td>\n",
       "      <td>21.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Accuracy</td>\n",
       "      <td>3013</td>\n",
       "      <td>21.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Communication</td>\n",
       "      <td>3008</td>\n",
       "      <td>21.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Cleanliness</td>\n",
       "      <td>3007</td>\n",
       "      <td>21.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Rating</td>\n",
       "      <td>2994</td>\n",
       "      <td>21.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Last Review</td>\n",
       "      <td>2865</td>\n",
       "      <td>20.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>First Review</td>\n",
       "      <td>2864</td>\n",
       "      <td>20.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Reviews per Month</td>\n",
       "      <td>2864</td>\n",
       "      <td>20.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Thumbnail Url</td>\n",
       "      <td>2560</td>\n",
       "      <td>18.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Medium Url</td>\n",
       "      <td>2560</td>\n",
       "      <td>18.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XL Picture Url</td>\n",
       "      <td>2560</td>\n",
       "      <td>18.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Response Rate</td>\n",
       "      <td>1772</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Response Time</td>\n",
       "      <td>1772</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neighbourhood Group Cleansed</td>\n",
       "      <td>830</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Summary</td>\n",
       "      <td>551</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Zipcode</td>\n",
       "      <td>474</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>State</td>\n",
       "      <td>119</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Amenities</td>\n",
       "      <td>103</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Market</td>\n",
       "      <td>56</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bathrooms</td>\n",
       "      <td>51</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Beds</td>\n",
       "      <td>46</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Location</td>\n",
       "      <td>40</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bedrooms</td>\n",
       "      <td>25</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Picture Url</td>\n",
       "      <td>19</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Description</td>\n",
       "      <td>5</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Verifications</td>\n",
       "      <td>5</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Calculated host listings count</td>\n",
       "      <td>4</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Since</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Total Listings Count</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Listings Count</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Picture Url</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Thumbnail Url</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Host Name</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>City</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Name</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Country</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Features</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Missing Values  % of Total Values\n",
       "Has Availability                         14049              99.92\n",
       "Host Acceptance Rate                     14027              99.77\n",
       "Jurisdiction Names                       13858              98.56\n",
       "License                                  13734              97.68\n",
       "Square Feet                              13487              95.92\n",
       "Monthly Price                            10603              75.41\n",
       "Weekly Price                             10490              74.61\n",
       "Notes                                     8676              61.71\n",
       "Security Deposit                          8091              57.55\n",
       "Interaction                               6100              43.39\n",
       "Access                                    6022              42.83\n",
       "Cleaning Fee                              5755              40.93\n",
       "Transit                                   5298              37.68\n",
       "Neighborhood Overview                     5266              37.45\n",
       "Host About                                4981              35.43\n",
       "Neighbourhood                             4900              34.85\n",
       "House Rules                               4886              34.75\n",
       "Space                                     3683              26.19\n",
       "Host Neighbourhood                        3614              25.70\n",
       "Review Scores Value                       3028              21.54\n",
       "Review Scores Location                    3028              21.54\n",
       "Review Scores Checkin                     3024              21.51\n",
       "Review Scores Accuracy                    3013              21.43\n",
       "Review Scores Communication               3008              21.39\n",
       "Review Scores Cleanliness                 3007              21.39\n",
       "Review Scores Rating                      2994              21.29\n",
       "Last Review                               2865              20.38\n",
       "First Review                              2864              20.37\n",
       "Reviews per Month                         2864              20.37\n",
       "Thumbnail Url                             2560              18.21\n",
       "Medium Url                                2560              18.21\n",
       "XL Picture Url                            2560              18.21\n",
       "Host Response Rate                        1772              12.60\n",
       "Host Response Time                        1772              12.60\n",
       "Neighbourhood Group Cleansed               830               5.90\n",
       "Summary                                    551               3.92\n",
       "Zipcode                                    474               3.37\n",
       "State                                      119               0.85\n",
       "Amenities                                  103               0.73\n",
       "Market                                      56               0.40\n",
       "Bathrooms                                   51               0.36\n",
       "Beds                                        46               0.33\n",
       "Host Location                               40               0.28\n",
       "Bedrooms                                    25               0.18\n",
       "Picture Url                                 19               0.14\n",
       "Description                                  5               0.04\n",
       "Host Verifications                           5               0.04\n",
       "Calculated host listings count               4               0.03\n",
       "Host Since                                   3               0.02\n",
       "Host Total Listings Count                    3               0.02\n",
       "Host Listings Count                          3               0.02\n",
       "Host Picture Url                             3               0.02\n",
       "Host Thumbnail Url                           3               0.02\n",
       "Host Name                                    3               0.02\n",
       "City                                         2               0.01\n",
       "Name                                         1               0.01\n",
       "Country                                      1               0.01\n",
       "Features                                     1               0.01"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_table(full_airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de llevar a cabo el split entre Trainintg y Test eliminamos las columnas que tenemos claro que no van a ser utiles para nuestro objetivo:\n",
    "1. **Las que contienen URLs**: Listing Url: drop_url  \n",
    "2. **Los Ids y lo relativo al Scrape realizado**: drop_id_scrape\n",
    "3. **Nombres y comentarios**:drop_comments\n",
    "4. **Direcciones**: A la vista de la informacion que contienen las variables de direccion podemos dropear varias de ellas por diversos motivos (sin que tengamos que dividir previamente en Train Test). Demasiado genericas: City, State, Market, Smart Location, Country Code, Country, Jurisdiction Names. Demasiado concretas: Street, Latitude, Longitude y Geolocation. Demasiado ruido o demasiados registros nulos: Neighbourhood, Host Location, Host Neighbourhood. Por ultimo Zipcode es una variable que para representar la direccion no me parece la mas adecuada debido a que aunque es un numero, deberia ser tratado como una variable categorica. Ademas contiene bastante ruido, una cantidad no despreciable de nulos y tambien es demasiado concreta (506 valores unicos)     Por tanto y para la evaluacion del modelo deberemos barajar cual de las dos opciones que quedan es la mejor para representar la \"zona\" en la que se encuentra el piso ( Neighbourhood Cleansed o Neighbourhood Group Cleansed, las cuales obviamente van a estar fuertemente correladas), pero esto debera hacerse una vez separado el dataset, para que los datos de Test no influyan en la decision. (En cualquier caso sera necesario hacer un trabajo de limpieza y categorizacion con la variable elegida):drop_address\n",
    "5. **Informascion relativa al Hospedador**: drop_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_url = np.array(['Listing Url', 'Thumbnail Url', 'Medium Url', 'Picture Url', 'XL Picture Url', 'Host URL',\n",
    "                     'Host Thumbnail Url','Host Picture Url'])\n",
    "full_airbnb.drop(drop_url, axis=1, inplace=True)\n",
    "\n",
    "drop_id_scrape = np.array(['ID', 'Scrape ID', 'Last Scraped', 'Host ID', 'Calendar last Scraped'])\n",
    "full_airbnb.drop(drop_id_scrape, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "drop_comments = np.array(['Name', 'Summary', 'Space', 'Description', 'Neighborhood Overview', 'Notes',\n",
    "                     'Transit','Access', 'Interaction', 'House Rules', 'Host Name', 'Experiences Offered',\n",
    "                         'Host About', 'Amenities', 'Features'])\n",
    "\n",
    "full_airbnb.drop(drop_comments, axis=1, inplace=True)\n",
    "\n",
    "drop_address = np.array(['Host Location', 'Host Neighbourhood', 'Neighbourhood', 'Street', 'Zipcode', \n",
    "    'City', 'State', 'Market', 'Smart Location','Country Code', 'Country', 'Latitude', \n",
    "                         'Longitude', 'Jurisdiction Names', 'Geolocation'])\n",
    "\n",
    "full_airbnb.drop(drop_address, axis=1, inplace=True)\n",
    "\n",
    "drop_host = np.array(['Host Since', 'Host Response Time', 'Host Response Rate', 'Host Acceptance Rate', \n",
    "    'Host Listings Count', 'Host Total Listings Count','Host Verifications', 'Calculated host listings count'])\n",
    "\n",
    "full_airbnb.drop(drop_host, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo eliminamos varios campos sueltos por los siguientes motivos:\n",
    "* **Square Feet**: Contiene 96% observaciones null\n",
    "* **Weekly Price**: Contiene 76% observaciones null\n",
    "* **Monthly Price**: Contiene 76% obsevaciones null\n",
    "* **Has Availability**: Contiene 99% observaciones null\n",
    "* **First Review**: No creemos que aporte informacion util para el calculo del precio\n",
    "* **Last Review**: No creemos que aporte informacion util para el calculo del precio\n",
    "* **Calendar Updated**: No creemos que aporte informacion util para el calculo del precio\n",
    "* **License**: Contiene 98% observaciones null\n",
    "* **Bed Type**: Casi el 99% de las camas son del mismo tipo (Real Bed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_varios = np.array(['Square Feet', 'Weekly Price', 'Monthly Price', 'Has Availability', 'First Review', 'Last Review',\n",
    "                     'Calendar Updated','License', 'Bed Type'])\n",
    "\n",
    "full_airbnb.drop(drop_varios, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14060, 29)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_airbnb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your slelected dataframe has 29 columns.\n",
      "There are 14 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Security Deposit</td>\n",
       "      <td>8091</td>\n",
       "      <td>57.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cleaning Fee</td>\n",
       "      <td>5755</td>\n",
       "      <td>40.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Location</td>\n",
       "      <td>3028</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Value</td>\n",
       "      <td>3028</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Checkin</td>\n",
       "      <td>3024</td>\n",
       "      <td>21.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Accuracy</td>\n",
       "      <td>3013</td>\n",
       "      <td>21.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Communication</td>\n",
       "      <td>3008</td>\n",
       "      <td>21.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Cleanliness</td>\n",
       "      <td>3007</td>\n",
       "      <td>21.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Review Scores Rating</td>\n",
       "      <td>2994</td>\n",
       "      <td>21.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Reviews per Month</td>\n",
       "      <td>2864</td>\n",
       "      <td>20.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neighbourhood Group Cleansed</td>\n",
       "      <td>830</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bathrooms</td>\n",
       "      <td>51</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Beds</td>\n",
       "      <td>46</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bedrooms</td>\n",
       "      <td>25</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Missing Values  % of Total Values\n",
       "Security Deposit                        8091              57.55\n",
       "Cleaning Fee                            5755              40.93\n",
       "Review Scores Location                  3028              21.54\n",
       "Review Scores Value                     3028              21.54\n",
       "Review Scores Checkin                   3024              21.51\n",
       "Review Scores Accuracy                  3013              21.43\n",
       "Review Scores Communication             3008              21.39\n",
       "Review Scores Cleanliness               3007              21.39\n",
       "Review Scores Rating                    2994              21.29\n",
       "Reviews per Month                       2864              20.37\n",
       "Neighbourhood Group Cleansed             830               5.90\n",
       "Bathrooms                                 51               0.36\n",
       "Beds                                      46               0.33\n",
       "Bedrooms                                  25               0.18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_table(full_airbnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14060.000000\n",
       "mean        62.066145\n",
       "std         37.643521\n",
       "min         11.000000\n",
       "25%         32.000000\n",
       "50%         55.000000\n",
       "75%         80.000000\n",
       "max        199.000000\n",
       "Name: Price, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_airbnb['Price'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tras la limpeza inicial nos hemos quedado con 28 caracteristicas (y el target).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación Train Test\n",
    "\n",
    "Ahora, y antes de pasar al analisis exploratorio y la seleccion de caratresiticas procemos a la divsion del dataset de entrada en train y test. De momento y por si llevamos a cabo tratamiento de outliers no separaremos la variable objetivo del resto de caracteristicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset de training: (11248, 29)\n",
      "Dimensiones del dataset de test: (2812, 29)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(full_airbnb, test_size=0.2, shuffle=True, random_state=0)\n",
    "print(f'Dimensiones del dataset de training: {train.shape}')\n",
    "print(f'Dimensiones del dataset de test: {test.shape}')\n",
    "# Guardamos\n",
    "train.to_csv('./data/train.csv', sep=';', decimal='.', index=False)\n",
    "test.to_csv('./data/test.csv', sep=';', decimal='.', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tengamos en cuenta que a partir de ahora todo el analisis exploratorio y selección de caracteristicas se hara sobre el dataset de entrenamiento.**\n",
    "**Posteriormente todas transformaciones llevadas a cabo en dicho dataset se deberan implementar en el de Test.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos tratando las variables de vecindario (Neighbourhood Cleansed y Neighbourhood Group Cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(full_airbnb['Neighbourhood'].unique())\n",
    "print(len(train['Neighbourhood Cleansed'].unique()))\n",
    "print(train['Neighbourhood Cleansed'].isna().sum())\n",
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    print(train['Neighbourhood Cleansed'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(full_airbnb['Neighbourhood'].unique())\n",
    "print(len(train['Neighbourhood Group Cleansed'].unique()))\n",
    "print(train['Neighbourhood Group Cleansed'].isna().sum())\n",
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    print(train['Neighbourhood Group Cleansed'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de los datos anteriores tenemos la siguiente informacio:\n",
    "- La variable \"Neighbourhood Cleansed\" contiene 449 categorias y ningun valor null.\n",
    "- La variable \"Neighbourhood Group Cleansed contiene 49 categorias y 819 celdas null.En lugar de imputar los nulls a la categoria \"Other\" lo rellenamos con el valor de la columna \"Neighbourhood Cleansed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Neighbourhood Group Cleansed'].fillna(train['Neighbourhood Cleansed'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación trataremos las otras tres variables categoricas que nos quedan (Property Type, Room Type y Cancellation Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(full_airbnb['Neighbourhood'].unique())\n",
    "print(len(train['Neighbourhood Group Cleansed'].unique()))\n",
    "print(train['Neighbourhood Group Cleansed'].isna().sum())\n",
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    print(train['Neighbourhood Group Cleansed'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es cierto que ahora hemos pasado de 49 categorias a 277, pero creemos que esta distribucion es mas realista que la de incluir tantos pisos en la categoria de Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train['Property Type'].value_counts())\n",
    "print(train['Room Type'].value_counts())\n",
    "print(train['Cancellation Policy'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dejar las categorias de Property Type en \"Apartment\", \"House\", \"Condominium\", \"Bed & Breakfast\", \"Loft\", \"Dorm\", \"Guesthouse\", \"Chalet\", \"Townhouse\", \"Hostel\" y \"Villa\" que representan mas del 95% del total y todas las demas las incluiremos en la categoría \"Other\".\n",
    "Para ello usaremos la funcion change_cat_to_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_main_cat_property_type = ['Apartment', 'House', 'Condominium', 'Bed & Breakfast', 'Loft', 'Dorm', 'Guesthouse',\n",
    "                               'Chalet', 'Townhouse', 'Hostel', 'Villa']\n",
    "train['Property Type'] = change_cat_to_other(array_main_cat_property_type, train['Property Type'])\n",
    "train['Property Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera vamos a dejar las categorias de Cancellation Policy en \"strict\", \"flexible\" y \"moderate\", que representan mas del 96% del total y todas las demas las incluiremos en la categoría \"Other\".\n",
    "Usamos nuevamente la funcion change_cat_to_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_main_cat_cancellation_policy = ['strict', 'flexible', 'moderate']\n",
    "train['Cancellation Policy'] = change_cat_to_other(array_main_cat_cancellation_policy, train['Cancellation Policy'])\n",
    "train['Cancellation Policy'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificacion de las variables categoricas (Mean encoding)\n",
    "Una vez reducidas las categorias de las variables categoricas, las convertiremos en numericas mediante el mecanismo de mean\n",
    "encoding.\n",
    "Guardamos las transformacion hechas en Train para reproducirlas en Test con un replace o un map sin volver a aplicar el mean encoding para evitar que los datos de test infulyan en el modelo.\n",
    "Para aplicar le metodo Mean Encoding es conveniente que no haya NaNs en la variable Target (Price), por ello imputaremos esos valores con la media de los precios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['Price'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mean = np.mean(train['Price'])\n",
    "train['Price'] = train['Price'].fillna(y_train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['Price'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Property Type\n",
    "mean_encode_property_type = train.groupby('Property Type')['Price'].mean()\n",
    "train.loc[:,'Property Type'] = train['Property Type'].map(mean_encode_property_type)\n",
    "\n",
    "#Cancellation Policy \n",
    "mean_encode_cancellation_policy = train.groupby('Cancellation Policy')['Price'].mean()\n",
    "train.loc[:,'Cancellation Policy'] = train['Cancellation Policy'].map(mean_encode_cancellation_policy)\n",
    "\n",
    "#Room Type\n",
    "mean_encode_room_type = train.groupby('Room Type')['Price'].mean()\n",
    "train.loc[:,'Room Type'] = train['Room Type'].map(mean_encode_room_type)\n",
    "\n",
    "#Neighbourhood Cleansed\n",
    "mean_encode_neigh = train.groupby('Neighbourhood Cleansed')['Price'].mean()\n",
    "train.loc[:,'Neighbourhood Cleansed'] = train['Neighbourhood Cleansed'].map(mean_encode_neigh)\n",
    "\n",
    "#Neighbourhood Group Cleansed\n",
    "mean_encode_neigh_group = train.groupby('Neighbourhood Group Cleansed')['Price'].mean()\n",
    "train.loc[:,'Neighbourhood Group Cleansed'] = train['Neighbourhood Group Cleansed'].map(mean_encode_neigh_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encode_property_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de pasar a la correlacion de variables llevemos a cabo un análisis rapido de las caracteristicas que nos quedan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Availability 30'].value_counts()\n",
    "train['Availability 30'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlacion de variables\n",
    "Tengamos en cuenta que "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = np.abs(train.drop(['Price'], axis=1).corr())\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n",
    "            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pintemos algunas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "train['Accommodates'].plot.hist(alpha=0.75, bins=25, grid = True)\n",
    "#plt.axis([0, 10, 0, 10000])\n",
    "plt.xlabel('Accommodates')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "train['Bedrooms'].plot.hist(alpha=0.5, bins=15, grid = True)\n",
    "plt.xlabel('Bedrooms')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "train['Beds'].plot.hist(alpha=0.5, bins=25, grid = False)\n",
    "plt.xlabel('Beds')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "train['Bathrooms'].plot.hist(alpha=0.5, bins=25, grid = False)\n",
    "plt.xlabel('Bathrooms')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de la grafica de correlacion y siendo un poco generosos con las variables a eliminar para simplificar el modelo, podemos deducir lo siguiente:\n",
    "1. **Accommodates** tiene una fuerte correlacion con Bedrooms, Beds y moderada con Guests Included\n",
    "2. **Availability 30** tiene fuerte correlacion con Availability 60 Availability 90 y moderada con Availability 365\n",
    "3. **Review Scores Rating** tiene una fuerte correlacion con Review Scores Accuracy, Review Scores Cleanliness, Review Scores Checkin, Review Scores Communication, Review Scores Value y moderada con Review Scores Location.\n",
    "4. **Number of Reviews** tiene fuerte  correlacion con Reviews per Month\n",
    "5. **Neighbourhood Cleansed y Neighbourhood Group Cleansed** muestran una fuerte correlacion, pero de momento dejamos las dos para analizar mediante el filtrado de caracterisdtcas cual de las dos influye mas en la regresion que tenemos que plantear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a eliminar todas estas variables del dataset de entrenamiento\n",
    "drop_corr = ['Bedrooms', 'Beds', 'Guests Included', 'Availability 60', 'Availability 90', 'Availability 365',\n",
    "             'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication',\n",
    "             'Review Scores Value', 'Review Scores Location', 'Reviews per Month']\n",
    "train.drop(drop_corr, axis=1, inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion variable dependiente \n",
    "Separemos el dataset train entre la varible dependiente (y_train) y el resto de variables independientes (X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Price']\n",
    "X_train= train.drop(['Price'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es aplicar los metodos de filtrados para regresion, pero esto solo se pueden aplicar sobre variables que no contienen valores NaN.\n",
    "por tanto analicemos que variables tienen valores NaN y llevemos a cabo el proceso de imputacion, aprovechare para analizar algun posible outlier\n",
    "\n",
    "Para hacerlo vamos a aplicar los siguientes comandos a todas las variables que quedan:\n",
    "```\n",
    "print(len(train['campo'].unique()))\n",
    "print(train['campo'].isna().sum())\n",
    "print(train['campo'].value_counts())\n",
    "\n",
    "```\n",
    "\n",
    "Haciendo esto vemos que los unicos campos que contienen NaNs son:\n",
    "- Bathroom (45) Imputamos la media\n",
    "- Security Deposit (6798). Es mas de la mitad de los regitros por lo que lo eliminamos\n",
    "- Cleaning Fee (4857). Imputamnos un cero. asumismos que quien no tiene ese dato es porque no hay gastos de limpieza.\n",
    "- Review Scores Rating (2651). Imputamos la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('Security Deposit', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bathroom = np.mean(train['Bathrooms'])\n",
    "mean_review = np.mean(train['Review Scores Rating'])\n",
    "mean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Bathrooms'] = train['Bathrooms'].fillna(mean_bathroom)\n",
    "X_train['Cleaning Fee'] = train['Cleaning Fee'].fillna(0)\n",
    "X_train['Review Scores Rating'] = train['Review Scores Rating'].fillna(mean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable objetivo\n",
    "Por ultimo analizamos la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train.unique()))\n",
    "print(y_train.isna().sum())\n",
    "print(y_train.value_counts())\n",
    "print(np.mean(y_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado para regresion\n",
    "Con las 13 variables que me quedan aplico los metodos de fitrado **f_regresion** y **mutual_info_regresion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "f_test, _ = f_regression(X_train, y_train)\n",
    "f_test /= np.max(f_test)\n",
    "mi = mutual_info_regression(X_train, y_train)\n",
    "mi /= np.max(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos esta informacion en forma de grafica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "featureNames = list(X_train.columns)\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(X_train.shape[1]),f_test,  align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]),featureNames, rotation = 90)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('$F-test$ score')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(X_train.shape[1]),mi,  align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]),featureNames, rotation = 90)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('Mutual information score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de las graficas y de los valores de f_test y mi las variables que mas estan impactando en la variable objetivo son:\n",
    "- Neighbourhood Cleansed (quitamos la grupal pues como ya habiamos visto esta fuertemente correlada con esta)\n",
    "- Room Type\n",
    "- Accommodates\n",
    "- Bathrooms\n",
    "- Cleaning Fee\n",
    "- Extra People\n",
    "- Minimum Nigths\n",
    "- Availability 30\n",
    "- cancelation Policy\n",
    "\n",
    "Como era de esperar el barrio, el tipo de habitacion y las personas que pueden ocupar la casa son los parametros que mas afectan al precio de la casa.\n",
    "\n",
    "**Me gustaria apuntar aqui que inicialmente habia hecho una prueba haciendo limpieza de la varibale \"Neighbourhood Cleansed\" poniendo en la categoria 'Others' todos aquellos barrios por debajo de una frecuencia determinada y la influencia de esa variable era mucho menor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto seran estas las variables que usare para testear mis modelos, elimando el resto del dataset de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_filtrado = ['Neighbourhood Group Cleansed', 'Property Type','Maximum Nights', 'Number of Reviews', 'Review Scores Rating']\n",
    "X_train.drop(drop_filtrado, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo cambiare el nombre de la variable \"Neighbourhood Group Cleansed\" por \"Barrio\" para que sea mas manejable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.rename(columns={'Neighbourhood Cleansed':'Barrio'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacion\n",
    "Por ultimo llevamos a cabo el escalado mediante StandarEscales\n",
    "Recordemos que para test deberemos usar el scaler obtenido en train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "XtrainScaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainScaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Y hasta aqui el analisis exploratorio y la limpieza del dataset, empezamos con las pruebas de los modelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparacion datos Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de iniciar la evaluacion de modelos, vamos a dejar preparado el **dataset de Test** con las mismas transformaciones que hemos llevado a cao sobre el de Train.\n",
    "Abajo indicamos el listado de dichas transformaciones para no olvidarnos de ninguna:\n",
    "- Lo primero es el dropeo de las variables que no van a participar en el modelo (correlacion y filtrado): drop_corr y drop_filtrado.\n",
    "- Tratamiento de las variables categoricas que influyen en el modelo (filtrado de categorias y Encoder): encoder_room_type y encoder_neigh_group\n",
    "- Tratamiento de NaNs: Bathroom y Cleaning Fee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y limpiamos el dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/test.csv',sep=';', decimal='.')\n",
    "test.shape\n",
    "#test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de los datos de arriba vemos que Price tienen valores NaN(3), por ello deberemos limpiarlos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropeamos\n",
    "test.drop(drop_corr, axis=1, inplace=True)\n",
    "test.drop(drop_filtrado, axis=1, inplace=True)\n",
    "\n",
    "#Eliminamos tambien la variable Security Deposit \n",
    "test.drop('Security Deposit', axis=1, inplace=True)\n",
    "\n",
    "#imputamos los valores NaNs del target con la media del train (y_train_mean)\n",
    "media_target_test = np.mean(test['Price'])\n",
    "test['Price'] = test['Price'].fillna(y_train_mean)\n",
    "\n",
    "#Imputamos los NaNs de la misma manera que se hace en Train\n",
    "test['Bathrooms'] = test['Bathrooms'].fillna(mean_bathroom)\n",
    "test['Cleaning Fee'] = test['Cleaning Fee'].fillna(0)\n",
    "\n",
    "\n",
    "#Aplicamos el Mean Encoder a \"Room Type\", \"Neighbourhood Cleansed\" y \"Cancellation Policy\"  el obtenido en train, si aparece alguna categoria nueva en test\n",
    "#lo trataremos a posteriori\n",
    "#mean_encode_room_type_test = test.groupby('Room Type')['Price'].mean()\n",
    "#mean_encode_neigh_test = test.groupby('Neighbourhood Cleansed')['Price'].mean()\n",
    "#mean_encode_cancellation_policy_test = test.groupby('Cancellation Policy')['Price'].mean()\n",
    "test.loc[:,'Room Type'] = test['Room Type'].map(mean_encode_room_type)\n",
    "test.loc[:,'Neighbourhood Cleansed'] = test['Neighbourhood Cleansed'].map(mean_encode_neigh)\n",
    "test.loc[:,'Cancellation Policy'] = test['Cancellation Policy'].map(mean_encode_cancellation_policy)\n",
    "\n",
    "\n",
    "#Por ultimo cambiamos el nombre de la variable a Barrio\n",
    "test.rename(columns={'Neighbourhood Cleansed':'Barrio'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_values_table(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como quedan valores nulos en Barrio y Cancellation Policy debido a aplicar sobre ellas el Mean Encoding de train, lo rellenaremos con la media de las medias obtenidas para esas categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mean_encode_neigh = np.mean(mean_encode_neigh)\n",
    "mean_mean_encode_cancellation_policy = np.mean(mean_encode_cancellation_policy)\n",
    "mean_mean_encode_cancellation_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Barrio'].fillna(mean_mean_encode_neigh, inplace=True)\n",
    "test['Cancellation Policy'].fillna(mean_mean_encode_cancellation_policy, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos la variable objetivo del tets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['Price']\n",
    "X_test= test.drop(['Price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los datos de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtestScaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion Logaritmica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Model\n",
    "Por convenio se recomienda empezar con una regresion regularizada tipo Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la regresion lineal (grado 1) no es necesario hacer el fit_transform de PolynomialFeatures, pudiendo usar directamente los datos de x_train_array y x_test_array. Pero tampoco pasa nada por aplicarselo poniendo degree=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos desde degree 1 hasta 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "degree = 2\n",
    "x_train_array = X_train.values\n",
    "x_test_array = X_test.values\n",
    "y_train_array = y_train.values\n",
    "y_test_array = y_test.values\n",
    "\n",
    "# features\n",
    "poly    = PolynomialFeatures(degree) # generamos x^j\n",
    "# Sin normalizar\n",
    "x_train_array_poly = poly.fit_transform(x_train_array)\n",
    "x_test_array_poly = poly.fit_transform(x_test_array)\n",
    "\n",
    "# Normalizado\n",
    "x_train_array_scaled_poly = poly.fit_transform(XtrainScaled)\n",
    "x_test_array_scaled_poly = poly.fit_transform(XtestScaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array_scaled_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "alpha_vector = np.logspace(-3,2,20)\n",
    "param_grid = {'alpha': alpha_vector }\n",
    "print(f'Valores de alpha: {alpha_vector}')\n",
    "grid = GridSearchCV(Ridge(), scoring= 'neg_mean_squared_error', param_grid=param_grid, cv = 5)\n",
    "grid.fit(x_train_array_scaled_poly, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(-1*(grid.best_score_)))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "scores = -1*np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.semilogx(alpha_vector,scores,'-o')\n",
    "plt.xlabel('alpha',fontsize=16)\n",
    "plt.ylabel('5-Fold MSE')\n",
    "#plt.ylim((0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo ahora con el valor óptimo de $\\alpha$ que hemos encontrado con validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_optimo = grid.best_params_['alpha']\n",
    "ridge = Ridge(alpha = alpha_optimo).fit(x_train_array_scaled_poly,y_train)\n",
    "\n",
    "ytrainRidge = ridge.predict(x_train_array_scaled_poly)\n",
    "ytestRidge  = ridge.predict(x_test_array_scaled_poly)\n",
    "mseTrainModelRidge = mean_squared_error(y_train,ytrainRidge)\n",
    "mseTestModelRidge = mean_squared_error(y_test,ytestRidge)\n",
    "\n",
    "print('MSE Modelo Ridge (train): %0.3g' % mseTrainModelRidge)\n",
    "print('MSE Modelo Ridge (test) : %0.3g' % mseTestModelRidge)\n",
    "\n",
    "print('RMSE Modelo Ridge (train): %0.3g' % np.sqrt(mseTrainModelRidge))\n",
    "print('RMSE Modelo Ridge (test) : %0.3g' % np.sqrt(mseTestModelRidge))\n",
    "\n",
    "print(f\"Score Train R\\u00b2 = {ridge.score(x_train_array_scaled_poly,y_train)} \")\n",
    "print(f\"Score Test R\\u00b2 = {ridge.score(x_test_array_scaled_poly,y_test)} \")\n",
    "\n",
    "feature_names = X_test.columns[0:]\n",
    "w = ridge.coef_\n",
    "for f,wi in zip(feature_names,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras probar desde grado uno (lineal) hasta grado cinco (polinomico) la regresion que mejor se ajusta es la de grado 2.\n",
    "Y se comprueba que la normalizacion de los datos apenas influye en la bondad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha_vector = np.logspace(-2,1,20)\n",
    "param_grid = {'alpha': alpha_vector }\n",
    "grid = GridSearchCV(Lasso(), scoring= 'neg_mean_squared_error', param_grid=param_grid, cv = 5)\n",
    "grid.fit(x_train_array_scaled_poly, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "#-1 porque es negado\n",
    "scores = -1*np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.semilogx(alpha_vector,scores,'-o')\n",
    "plt.xlabel('alpha',fontsize=16)\n",
    "plt.ylabel('5-Fold MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_optimo = grid.best_params_['alpha']\n",
    "lasso = Lasso(alpha = alpha_optimo).fit(x_train_array_scaled_poly,y_train)\n",
    "\n",
    "ytrainLasso = lasso.predict(x_train_array_scaled_poly)\n",
    "ytestLasso  = lasso.predict(x_test_array_scaled_poly)\n",
    "mseTrainModelLasso = mean_squared_error(y_train,ytrainLasso)\n",
    "mseTestModelLasso = mean_squared_error(y_test,ytestLasso)\n",
    "\n",
    "print('MSE Modelo Lasso (train): %0.3g' % mseTrainModelLasso)\n",
    "print('MSE Modelo Lasso (test) : %0.3g' % mseTestModelLasso)\n",
    "\n",
    "print('RMSE Modelo Lasso (train): %0.3g' % np.sqrt(mseTrainModelLasso))\n",
    "print('RMSE Modelo Lasso (test) : %0.3g' % np.sqrt(mseTestModelLasso))\n",
    "\n",
    "print(f\"Score Train R\\u00b2 = {lasso.score(x_train_array_scaled_poly,y_train)} \")\n",
    "print(f\"Score Test R\\u00b2 = {lasso.score(x_train_array_scaled_poly,y_test)} \")\n",
    "\n",
    "feature_names = X_train.columns[0:]\n",
    "w = lasso.coef_\n",
    "for f,wi in zip(feature_names,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como vemos, tanto los valores de Ridge como los de Lasso son muy parecidos **\n",
    "Como era de esperar el grado que mejores resultados da es el 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(y_train), bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de esta distribucion no parece que pueda tener mucho sentido aplicar una transformacion logaritmica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos primero el Crossvalidation con GridSearchCV, para encotrar los parametros libres optimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid search\n",
    "maxDepth = range(1,15)\n",
    "tuned_parameters = {'max_depth': maxDepth}\n",
    "\n",
    "grid = GridSearchCV(RandomForestRegressor(random_state=0, n_estimators=200, max_features='sqrt'), param_grid=tuned_parameters,cv=10) \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.plot(maxDepth,scores,'-o')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('10-fold ACC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos nuestro modelo con el parametro libre optimo obtenido en el paso anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDepthOptimo = grid.best_params_['max_depth']\n",
    "bagModel = RandomForestRegressor(max_depth=8,n_estimators=200,max_features='sqrt').fit(X_train,y_train)\n",
    "\n",
    "print(\"Score Train: \",bagModel.score(X_train,y_train))\n",
    "print(\"Score Test: \",bagModel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analizamos ahora las caracteristicas mas relevantes para nuestro modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "importances = bagModel.feature_importances_\n",
    "importances = importances / np.max(importances)\n",
    "\n",
    "feature_names = X_train.columns[0:]\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(range(X_train.shape[1]),importances[indices])\n",
    "plt.yticks(range(X_train.shape[1]),feature_names[indices])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como vemos las caracteristicas mas relevantes siguen practicamente el mismo orden que lo que habiamos obtenido \n",
    "mas arriba en las tareas de regularizacion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me permito por ultimo copiar esto para tenerlo todo en un unico notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "rf = RandomForestRegressor(oob_score = True)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = rf.predict(X_test)\n",
    "rf_testing_set_score = rf.score(X_test, y_test)\n",
    "rf_median_abs_error = median_absolute_error(y_test, y_predict)\n",
    "rf_mean_abs_error = mean_absolute_error(y_test, y_predict)\n",
    "print('R^2 en datos de test: ' + str(round(rf_testing_set_score,3)))\n",
    "print('Mediana del error en datos de test: ' + str(round(rf_median_abs_error,3)))\n",
    "print('Media del error en datos de test: ' + str(round(rf_mean_abs_error,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\n",
    "    \"n_estimators\": [100, 500, 1000, 2000]\n",
    "}\n",
    "\n",
    "rf_tuned = GridSearchCV(RandomForestRegressor(), cv = 3, param_grid = tuned_parameters)\n",
    "\n",
    "preds = rf_tuned.fit(X_train, y_train)\n",
    "best = rf_tuned.best_estimator_ \n",
    "y_predict = rf_tuned.predict(X_test)\n",
    "rft_testing_set_score = rf_tuned.score(X_test, y_test)\n",
    "rft_median_abs_error = median_absolute_error(y_test, y_predict)\n",
    "rf_mean_abs_error = mean_absolute_error(y_test, y_predict)\n",
    "\n",
    "print('Mejores parámetros:', rf_tuned.best_params_)\n",
    "print('R^2 en datos de test: ' + str(round(rf_testing_set_score,3)))\n",
    "print('Mediana del error en datos de test: ' + str(round(rf_median_abs_error,3)))\n",
    "print('Media del error en datos de test: ' + str(round(rf_mean_abs_error,3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
